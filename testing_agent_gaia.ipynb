{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681441cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools import BraveSearch\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from text_web_browser import (\n",
    "    ArchiveSearchTool,\n",
    "    FinderTool,\n",
    "    FindNextTool,\n",
    "    PageDownTool,\n",
    "    PageUpTool,\n",
    "    SimpleTextBrowser,\n",
    "    VisitTool,\n",
    ")\n",
    "from text_inspector_tool import TextInspectorTool\n",
    "from visual_qa import VisualQATool\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698bb65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# EXAMPLE COMMAND: from folder examples/open_deep_research, run: python run_gaia.py --concurrency 32 --run-name generate-traces-03-apr-noplanning --model-id gpt-4o\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import shutil\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from reformulator import prepare_response\n",
    "from run_agents import (\n",
    "    get_single_file_description,\n",
    "    get_zip_description,\n",
    ")\n",
    "from text_inspector_tool import TextInspectorTool\n",
    "from text_web_browser import (\n",
    "    ArchiveSearchTool,\n",
    "    FinderTool,\n",
    "    FindNextTool,\n",
    "    PageDownTool,\n",
    "    PageUpTool,\n",
    "    SimpleTextBrowser,\n",
    "    VisitTool,\n",
    ")\n",
    "from visual_qa import VisualQATool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from smolagents import (\n",
    "    CodeAgent,\n",
    "    GoogleSearchTool,\n",
    "    LiteLLMModel,\n",
    "    Model,\n",
    "    ToolCallingAgent,\n",
    ")\n",
    "from agent_with_new_tools import create_plan_and_execute_agent, PlanExecute\n",
    "from langchain.chat_models import init_chat_model\n",
    "from text_inspector_tool import TextInspectorTool\n",
    "from visual_qa import VisualQATool\n",
    "\n",
    "load_dotenv(override=True)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "append_answer_lock = threading.Lock()\n",
    "import sys\n",
    "sys.argv = ['']  # Override sys.argv to prevent argparse from processing notebook arguments\n",
    "\n",
    "import argparse\n",
    "\n",
    "### IMPORTANT: EVALUATION SWITCHES\n",
    "\n",
    "print(\"Make sure you deactivated any VPN like Tailscale, else some URLs will be blocked!\")\n",
    "\n",
    "custom_role_conversions = {\"tool-call\": \"assistant\", \"tool-response\": \"user\"}\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\"\n",
    "\n",
    "BROWSER_CONFIG = {\n",
    "    \"viewport_size\": 1024 * 5,\n",
    "    \"downloads_folder\": \"downloads_folder\",\n",
    "    \"request_kwargs\": {\n",
    "        \"headers\": {\"User-Agent\": user_agent},\n",
    "        \"timeout\": 300,\n",
    "    },\n",
    "    \"serpapi_key\": os.getenv(\"SERPAPI_API_KEY\"),\n",
    "}\n",
    "\n",
    "os.makedirs(f\"./{BROWSER_CONFIG['downloads_folder']}\", exist_ok=True)\n",
    "print('created new folder')\n",
    "def create_agent_team(model: Model):\n",
    "    text_limit = 100000\n",
    "    ti_tool = TextInspectorTool(model, text_limit)\n",
    "\n",
    "    browser = SimpleTextBrowser(**BROWSER_CONFIG)\n",
    "\n",
    "    WEB_TOOLS = [\n",
    "        GoogleSearchTool(provider=\"serper\"),\n",
    "        VisitTool(browser),\n",
    "        PageUpTool(browser),\n",
    "        PageDownTool(browser),\n",
    "        FinderTool(browser),\n",
    "        FindNextTool(browser),\n",
    "        ArchiveSearchTool(browser),\n",
    "        TextInspectorTool(model, text_limit),\n",
    "    ]\n",
    "\n",
    "    text_webbrowser_agent = ToolCallingAgent(\n",
    "        model=model,\n",
    "        tools=WEB_TOOLS,\n",
    "        max_steps=20,\n",
    "        verbosity_level=2,\n",
    "        planning_interval=4,\n",
    "        name=\"search_agent\",\n",
    "        description=\"\"\"A team member that will search the internet to answer your question.\n",
    "Ask him for all your questions that require browsing the web.\n",
    "Provide him as much context as possible, in particular if you need to search on a specific timeframe!\n",
    "And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.\n",
    "Your request must be a real sentence, not a google search! Like \"Find me this information (...)\" rather than a few keywords.\n",
    "\"\"\",\n",
    "        provide_run_summary=True,\n",
    "    )\n",
    "    text_webbrowser_agent.prompt_templates[\"managed_agent\"][\"task\"] += \"\"\"You can navigate to .txt online files.\n",
    "If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.\n",
    "Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information.\"\"\"\n",
    "\n",
    "    manager_agent = CodeAgent(\n",
    "        model=model,\n",
    "        tools=[VisualQATool, ti_tool],\n",
    "        max_steps=12,\n",
    "        verbosity_level=2,\n",
    "        additional_authorized_imports=[\"*\"],\n",
    "        planning_interval=4,\n",
    "        managed_agents=[text_webbrowser_agent],\n",
    "    )\n",
    "    return manager_agent\n",
    "\n",
    "def load_gaia_dataset(use_raw_dataset: bool, set_to_run: str) -> datasets.Dataset:\n",
    "    if not os.path.exists(\"data/gaia\"):\n",
    "        print('gaia dataset path doesnt exist')\n",
    "        if use_raw_dataset:\n",
    "            print('in use raw dataset')\n",
    "            snapshot_download(\n",
    "                repo_id=\"gaia-benchmark/GAIA\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "        else:\n",
    "            # WARNING: this dataset is gated: make sure you visit the repo to require access.\n",
    "            snapshot_download(\n",
    "                repo_id=\"smolagents/GAIA-annotated\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "\n",
    "    def preprocess_file_paths(row):\n",
    "        if len(row[\"file_name\"]) > 0:\n",
    "            row[\"file_name\"] = f\"data/gaia/{set_to_run}/\" + row[\"file_name\"]\n",
    "        return row\n",
    "    \n",
    "    eval_ds = datasets.load_dataset(\n",
    "        \"data/gaia/GAIA.py\",\n",
    "        name=\"2023_all\",\n",
    "        split=set_to_run,\n",
    "    )\n",
    "\n",
    "    eval_ds = eval_ds.rename_columns({\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"})\n",
    "    eval_ds = eval_ds.map(preprocess_file_paths)\n",
    "    return eval_ds\n",
    "\n",
    "def append_answer(entry: dict, jsonl_file: str) -> None:\n",
    "    jsonl_path = Path(jsonl_file)\n",
    "    jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with append_answer_lock, open(jsonl_file, \"a\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(json.dumps(entry) + \"\\n\")\n",
    "    assert jsonl_path.exists(), \"File not found!\"\n",
    "    print(\"Answer exported to file:\", jsonl_path.resolve())\n",
    "\n",
    "def answer_single_question(\n",
    "    example: dict, model_id: str, answers_file: str, visual_inspection_tool: TextInspectorTool\n",
    ") -> None:\n",
    "    print('example', example)\n",
    "    model_params: dict[str, Any] = {\n",
    "        \"model_id\": model_id,\n",
    "        \"custom_role_conversions\": custom_role_conversions,\n",
    "    }\n",
    "    if model_id == \"o1\":\n",
    "        model_params[\"reasoning_effort\"] = \"high\"\n",
    "        model_params[\"max_completion_tokens\"] = 8192\n",
    "    else:\n",
    "        model_params[\"max_tokens\"] = 4096\n",
    "    model = LiteLLMModel(**model_params)\n",
    "    document_inspection_tool = TextInspectorTool(model, 100000)\n",
    "\n",
    "    agent = create_agent_team(model)\n",
    "\n",
    "    augmented_question = \"\"\"You have one question to answer. It is paramount that you provide a correct answer.\n",
    "Give it all you can: I know for a fact that you have access to all the relevant tools to solve it and find the correct answer (the answer does exist).\n",
    "Failure or 'I cannot answer' or 'None found' will not be tolerated, success will be rewarded.\n",
    "Run verification steps if that's needed, you must make sure you find the correct answer! Here is the task:\n",
    "\n",
    "\"\"\" + example[\"question\"]\n",
    "\n",
    "    if example[\"file_name\"]:\n",
    "        if \".zip\" in example[\"file_name\"]:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use these attached files:\\n\"\n",
    "            prompt_use_files += get_zip_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "        else:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use this attached file:\\n\"\n",
    "            prompt_use_files += get_single_file_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "        augmented_question += prompt_use_files\n",
    "\n",
    "    start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    try:\n",
    "        # Run agent üöÄ\n",
    "        final_result = agent.run(augmented_question)\n",
    "\n",
    "        agent_memory = agent.write_memory_to_messages()\n",
    "\n",
    "        final_result = prepare_response(augmented_question, agent_memory, reformulation_model=model)\n",
    "\n",
    "        output = str(final_result)\n",
    "        for memory_step in agent.memory.steps:\n",
    "            memory_step.model_input_messages = None\n",
    "        intermediate_steps = agent_memory\n",
    "\n",
    "        # Check for parsing errors which indicate the LLM failed to follow the required format\n",
    "        parsing_error = True if any([\"AgentParsingError\" in step for step in intermediate_steps]) else False\n",
    "\n",
    "        # check if iteration limit exceeded\n",
    "        iteration_limit_exceeded = True if \"Agent stopped due to iteration limit or time limit.\" in output else False\n",
    "        raised_exception = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error on \", augmented_question, e)\n",
    "        output = None\n",
    "        intermediate_steps = []\n",
    "        parsing_error = False\n",
    "        iteration_limit_exceeded = False\n",
    "        exception = e\n",
    "        raised_exception = True\n",
    "    end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    token_counts_manager = agent.monitor.get_total_token_counts()\n",
    "    token_counts_web = list(agent.managed_agents.values())[0].monitor.get_total_token_counts()\n",
    "    total_token_counts = {\n",
    "        \"input\": token_counts_manager[\"input\"] + token_counts_web[\"input\"],\n",
    "        \"output\": token_counts_manager[\"output\"] + token_counts_web[\"output\"],\n",
    "    }\n",
    "    annotated_example = {\n",
    "        \"agent_name\": model.model_id,\n",
    "        \"question\": example[\"question\"],\n",
    "        \"augmented_question\": augmented_question,\n",
    "        \"prediction\": output,\n",
    "        \"intermediate_steps\": intermediate_steps,\n",
    "        \"parsing_error\": parsing_error,\n",
    "        \"iteration_limit_exceeded\": iteration_limit_exceeded,\n",
    "        \"agent_error\": str(exception) if raised_exception else None,\n",
    "        \"task\": example[\"task\"],\n",
    "        \"task_id\": example[\"task_id\"],\n",
    "        \"true_answer\": example[\"true_answer\"],\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"token_counts\": total_token_counts,\n",
    "    }\n",
    "    append_answer(annotated_example, answers_file)\n",
    "\n",
    "def answer_single_question_V2(\n",
    "    example: dict, answers_file: str\n",
    ") -> None:\n",
    "    print('\\n\\n')\n",
    "    print('================================================================================================================')\n",
    "    print('example', example)\n",
    "\n",
    "    question = example[\"question\"]\n",
    "    document_inspection_tool = TextInspectorTool(\n",
    "            model=init_chat_model(\"gpt-4.1\", model_provider=\"openai\", temperature=0), \n",
    "            text_limit=10000)\n",
    "    visual_inspection_tool = VisualQATool(model=init_chat_model(\"gpt-4.1\", model_provider=\"openai\", temperature=0))\n",
    "\n",
    "    if example[\"file_name\"]:\n",
    "        if \".zip\" in example[\"file_name\"]:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use these attached files:\\n\"\n",
    "            prompt_use_files += get_zip_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "        else:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use this attached file:\\n\"\n",
    "            prompt_use_files += get_single_file_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "\n",
    "    \n",
    "        # if \".zip\" in example[\"file_name\"]:\n",
    "        #     folder_path = example[\"file_name\"].replace(\".zip\", \"\")\n",
    "        #     os.makedirs(folder_path, exist_ok=True)\n",
    "        #     shutil.unpack_archive(example[\"file_name\"], folder_path)\n",
    "        #     string_w_paths = \"\"\n",
    "        #     for root, dirs, files in os.walk(folder_path):\n",
    "        #         print('root', root)\n",
    "        #         print('dirs', dirs)\n",
    "        #         print('files', files)\n",
    "        #         for file in files:\n",
    "        #             file_path = os.path.join(root, file)\n",
    "        #             string_w_paths+= file_path+\"\\n\"\n",
    "        #     prompt_use_files = f\"\\n\\nTo solve the question above, you will have to use these attached files: {string_w_paths}\\n\" \n",
    "\n",
    "        # else:\n",
    "        #     prompt_use_files = f\"\\n\\nTo solve the question above, you will have to use these attached files: {example[\"file_name\"]}\\n\" \n",
    "        question += prompt_use_files\n",
    "    print(question)\n",
    "    return\n",
    "    # CREATE AGENT\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    graph = create_plan_and_execute_agent(\n",
    "        llm_name_planner=\"gpt-4.1-mini\",\n",
    "        llm_name_executor=\"gpt-4.1-mini\",\n",
    "        llm_name_replanner=\"gpt-4.1-mini\",\n",
    "        llm_name_answer=\"gpt-4.1-mini\")\n",
    "\n",
    "    initial_state = PlanExecute(\n",
    "        question=question,\n",
    "        plan=[],\n",
    "        intermediate_responses=[],\n",
    "        response=\"\",\n",
    "        current_step=0,\n",
    "        error_count=0,\n",
    "        validation=None,\n",
    "        agent_finished=False\n",
    "    )\n",
    "\n",
    "    start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    try: \n",
    "        # Run Agent\n",
    "        print('Question:', question)\n",
    "        workflow = initial_state\n",
    "        workflow =  graph.invoke(initial_state, config=config)\n",
    "        # print('QUESTION')\n",
    "        # print(workflow[\"question\"])\n",
    "        # print(\"STEPS\")\n",
    "        # previous_steps_with_answers = \"\\n\".join(\n",
    "        #     f\"{i}. INSTRUCTION: {workflow['plan'][i]} ANSWER: {workflow['intermediate_responses'][i]}\\n\" for i in range(len(workflow['intermediate_responses']))\n",
    "        # )\n",
    "        # print(previous_steps_with_answers)\n",
    "        # print(\"Current step: \", workflow[\"current_step\"])\n",
    "        # print(\"RESPONSE\")\n",
    "        # print(workflow[\"response\"])\n",
    "        # print('TRUE RESPONSE')\n",
    "        # print(example['true_answer'])\n",
    "        # response = \"No response was found\"\n",
    "        # for event in graph.stream(initial_state, config=config):\n",
    "        #     if \"planner\" in event:\n",
    "        #         print('PLAN')\n",
    "        #         for i, p in enumerate(event['planner']['plan']):\n",
    "        #             print(f\"Step {i}: {p}\")\n",
    "        #     elif \"agent\" in event:\n",
    "        #         print('AGENT')\n",
    "        #         print(event['agent']['intermediate_responses'])\n",
    "        #     elif \"replan\" in event:\n",
    "        #         if 'plan' in event['replan']:\n",
    "        #             print('REPLAN')\n",
    "        #             for i, p in enumerate(event['replan']['plan']):\n",
    "        #                 print(f\"Step {i}: {p}\")\n",
    "        #         else:\n",
    "        #             print('REPLAN ERROR')\n",
    "        #             print(event['replan'])\n",
    "        #     elif 'answer' in event:\n",
    "        #         response = event['answer']['response']\n",
    "        #         print('RESPONSE')\n",
    "        #         print(response)\n",
    "        #         break\n",
    "        # workflow = {}\n",
    "\n",
    "        raised_exception = False\n",
    "    except Exception as e: \n",
    "        print(\"Error on\", question, e)\n",
    "        exception = e\n",
    "        raised_exception = True\n",
    "\n",
    "    end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    annotated_example = {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"augmented_question\": question,\n",
    "        \"response\": workflow[\"response\"].replace('FINAL ANSWER: ', ''),\n",
    "        \"plan\": workflow[\"plan\"],\n",
    "        \"intermediate_responses\": workflow[\"intermediate_responses\"],\n",
    "        \"current_step\": workflow[\"current_step\"],\n",
    "        \"plan_with_answers\": \"\\n\".join(\n",
    "            f\"{i}. INSTRUCTION: {workflow['plan'][i]} ANSWER: {workflow['intermediate_responses'][i]}\\n\" for i in range(len(workflow['intermediate_responses']))\n",
    "        ),\n",
    "        \"agent_error\": str(exception) if raised_exception else None,\n",
    "        \"task\": example[\"task\"],\n",
    "        \"task_id\": example[\"task_id\"],\n",
    "        \"true_answer\": example[\"true_answer\"],\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "    }\n",
    "    print('-------------------------------------------------------------')\n",
    "    list_print = ['question', 'augmented_question', 'plan_with_answers', 'response', 'true_answer']\n",
    "    for k in list_print:\n",
    "        print(k)\n",
    "        print(annotated_example[k])\n",
    "    print('-------------------------------------------------------------')\n",
    "    # print('annotated_example')\n",
    "    # print(annotated_example)\n",
    "    append_answer(annotated_example, answers_file)\n",
    "\n",
    "def get_examples_to_answer(answers_file: str, eval_ds: datasets.Dataset) -> list[dict]:\n",
    "    print(f\"Loading answers from {answers_file}...\")\n",
    "    try:\n",
    "        done_questions = pd.read_json(answers_file, lines=True)[\"question\"].tolist()\n",
    "        print(f\"Found {len(done_questions)} previous results!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error when loading records: \", e)\n",
    "        print(\"No usable records! ‚ñ∂Ô∏è Starting new.\")\n",
    "        done_questions = []\n",
    "    return [line for line in eval_ds.to_list() if line[\"question\"] not in done_questions and line[\"file_name\"]]\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--concurrency\", type=int, default=8)\n",
    "    parser.add_argument(\"--model-id\", type=str, default=\"o1\")\n",
    "    parser.add_argument(\"--run-name\", type=str, default=\"test1\")\n",
    "    parser.add_argument(\"--set-to-run\", type=str, default=\"validation\")\n",
    "    parser.add_argument(\"--use-open-models\", type=bool, default=False)\n",
    "    parser.add_argument(\"--use-raw-dataset\", action=\"store_true\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    print('In main()')\n",
    "    args = parse_args()\n",
    "    print(f\"Starting run with arguments: {args}\")\n",
    "\n",
    "    eval_ds = load_gaia_dataset(args.use_raw_dataset, args.set_to_run)\n",
    "    print(\"Loaded evaluation dataset:\")\n",
    "    print(pd.DataFrame(eval_ds)[\"task\"].value_counts())\n",
    "\n",
    "    answers_file = f\"output/{args.set_to_run}/{args.run_name}.jsonl\"\n",
    "    tasks_to_run = get_examples_to_answer(answers_file, eval_ds)\n",
    "    print('answer file', answers_file)\n",
    "    # Sequential processing with tqdm progress bar\n",
    "\n",
    "    # example =  {'task_id': '5b2a14e8-6e59-479c-80e3-4696e8980152', 'question': 'The brand that makes these harnesses the dogs are wearing in the attached pic shares stories from their ambassadors on their website. What meat is mentioned in the story added Dec 8th 2022?', 'task': '3', 'true_answer': 'bacon', 'file_name': 'data/gaia/validation/5b2a14e8-6e59-479c-80e3-4696e8980152.jpg', 'file_path': '/Users/aymar/Documents/Perso_projects/Agents-course/unit4-basic-agent/tools/data/gaia/2023/validation/5b2a14e8-6e59-479c-80e3-4696e8980152.jpg', 'Annotator Metadata': {'Steps': '1. Use image search for \"dog harness brands with yellow logos\"\\n2. Look at harnesses until a similar harness shows up\\n3. Click through to see the harness\\n4. Search \"ruffwear\"\\n5. Go to the website\\n6. Navigate to stories\\n7. Find the story posted Dec 8th 2022\\n8. Read the story to find any meats mentioned', 'Number of steps': '8', 'How long did this take?': '15 minutes', 'Tools': '1. image recognition tools\\n2. image search tools\\n3. web browser\\n4. search engine', 'Number of tools': '4'}}\n",
    "    # answer_single_question_V2(example, answers_file)\n",
    "\n",
    "    for example in tqdm(tasks_to_run, desc=\"Processing tasks\"):\n",
    "        answer_single_question_V2(example, answers_file)\n",
    "\n",
    "    print(\"All tasks processed.\")\n",
    "print('beforemain')\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
