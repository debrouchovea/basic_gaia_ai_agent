{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681441cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# # EXAMPLE COMMAND: from folder examples/open_deep_research, run: python run_gaia.py --concurrency 32 --run-name generate-traces-03-apr-noplanning --model-id gpt-4o\n",
    "# import argparse\n",
    "# import json\n",
    "# import os\n",
    "# import threading\n",
    "# from datetime import datetime\n",
    "# from pathlib import Path\n",
    "# from typing import Any\n",
    "# import shutil\n",
    "\n",
    "# import datasets\n",
    "# import pandas as pd\n",
    "# from dotenv import load_dotenv\n",
    "# from huggingface_hub import login, snapshot_download\n",
    "# from tools.reformulator import prepare_response\n",
    "# from tools.run_agents import (\n",
    "#     get_single_file_description,\n",
    "#     get_zip_description,\n",
    "# )\n",
    "# from tools.text_inspector_tool import TextInspectorTool\n",
    "# from tools.text_web_browser import (\n",
    "#     ArchiveSearchTool,\n",
    "#     FinderTool,\n",
    "#     FindNextTool,\n",
    "#     PageDownTool,\n",
    "#     PageUpTool,\n",
    "#     SimpleTextBrowser,\n",
    "#     VisitTool,\n",
    "# )\n",
    "# from tools.visual_qa import VisualQATool\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from smolagents import (\n",
    "#     CodeAgent,\n",
    "#     GoogleSearchTool,\n",
    "#     LiteLLMModel,\n",
    "#     Model,\n",
    "#     ToolCallingAgent,\n",
    "# )\n",
    "# from tools.agent_with_tools import create_plan_and_execute_agent, PlanExecute\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# from tools.text_inspector_tool import TextInspectorTool\n",
    "# from tools.visual_qa import VisualQATool\n",
    "\n",
    "# # load_dotenv(override=True)\n",
    "# login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# append_answer_lock = threading.Lock()\n",
    "# import sys\n",
    "# sys.argv = ['']  # Override sys.argv to prevent argparse from processing notebook arguments\n",
    "\n",
    "# import argparse\n",
    "\n",
    "\n",
    "# import requests\n",
    "# from langchain.tools import tool\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "# from langchain_core.tools import Tool\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "# from langchain_community.tools import BraveSearch\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.utilities import SerpAPIWrapper\n",
    "# from langchain.agents import Tool, initialize_agent, AgentType\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from tools.text_web_browser import (\n",
    "#     ArchiveSearchTool,\n",
    "#     FinderTool,\n",
    "#     FindNextTool,\n",
    "#     PageDownTool,\n",
    "#     PageUpTool,\n",
    "#     SimpleTextBrowser,\n",
    "#     VisitTool,\n",
    "# )\n",
    "# from tools.text_inspector_tool import TextInspectorTool\n",
    "# from tools.visual_qa import VisualQATool\n",
    "# from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults\n",
    "# import pandas as pd\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# EXAMPLE COMMAND: from folder examples/open_deep_research, run:\n",
    "# python run_gaia.py --concurrency 32 --run-name generate-traces-03-apr-noplanning --model-id gpt-4o\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "from tools.reformulator import prepare_response\n",
    "from tools.run_agents import get_single_file_description, get_zip_description\n",
    "from tools.text_inspector_tool import TextInspectorTool\n",
    "from tools.text_web_browser import (\n",
    "    ArchiveSearchTool,\n",
    "    FinderTool,\n",
    "    FindNextTool,\n",
    "    PageDownTool,\n",
    "    PageUpTool,\n",
    "    SimpleTextBrowser,\n",
    "    VisitTool,\n",
    ")\n",
    "from tools.visual_qa import VisualQATool\n",
    "from tools.agent_with_tools import create_plan_and_execute_agent, PlanExecute\n",
    "\n",
    "from smolagents import (\n",
    "    CodeAgent,\n",
    "    GoogleSearchTool,  # make sure this is distinct from Serp/BSerp tools\n",
    "    LiteLLMModel,\n",
    "    Model,\n",
    "    ToolCallingAgent,\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model, ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, SerpAPIWrapper\n",
    "from langchain_community.tools import BraveSearch\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "append_answer_lock = threading.Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aab96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent executed step: {'messages': [SystemMessage(content='\\n            You are a helpful agent that should solve the instruct given to you by the user. You have multiple tools available to you. \\n\\n ', additional_kwargs={}, response_metadata={}, id='9a2e477a-815b-47be-9a1c-35b62a337f5f'), HumanMessage(content='\\n            The instruction is only a single step of a plan that solves a bigger problem. You should not return the final answer.\\n\\n            Your task is only to answer to the instruction given to you. You shouldn\\'t add other text than the answer.\\n\\n            The final goal of the plan is to solve the following question: \\n\\n            when is aymar de bergeyck born?\\n\\n            You need to solve this instruction: Search the web for \"Aymar de Bergeyck birth date\" to find reliable sources that provide his date of birth.\\n\\n            ', additional_kwargs={}, response_metadata={}, id='1bbf5cc7-dfbe-4799-ac4c-852486780214'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_lUUuVbkOPvNOUpHQv0N3TfNp', 'function': {'arguments': '{\"__arg1\":\"Aymar de Bergeyck birth date\"}', 'name': 'GoogleSerperAPIWrapper'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'service_tier': 'default'}, name='supervisor_agent', id='run--0752525f-07dd-4453-929e-115f900ddd4f-0', tool_calls=[{'name': 'GoogleSerperAPIWrapper', 'args': {'__arg1': 'Aymar de Bergeyck birth date'}, 'id': 'call_lUUuVbkOPvNOUpHQv0N3TfNp', 'type': 'tool_call'}]), ToolMessage(content='{\"searchParameters\": {\"q\": \"Aymar de Bergeyck birth date\", \"gl\": \"us\", \"hl\": \"en\", \"type\": \"search\", \"num\": 10, \"engine\": \"google\"}, \"organic\": [{\"title\": \"Aymar Anne de Liedekerke de Brouchoven de Bergeyck (1944 ...\", \"link\": \"https://www.findagrave.com/memorial/189541905/aymar-anne-de_brouchoven_de_bergeyck\", \"snippet\": \"Birth: 22 Apr 1944. Uccle, Arrondissement Brussel-Hoofdstad, Brussels-Capital Region, Belgium ; Death: 17 Oct 2004 (aged 60). Etterbeek, Arrondissement Brussel- ...\", \"position\": 1}, {\"title\": \"Family tree of Thomas DE BERGEYCK - Geneastar\", \"link\": \"https://en.geneastar.org/genealogy/debrouchovet/thomas-de-bergeyck\", \"snippet\": \"Thomas DE BROUCHOVEN DE BERGEYCK, Belgian journalist, Born on August 28, 1976 in Ixelles, Belgique, Belgium (48 years).\", \"position\": 2}, {\"title\": \"Aymar de Bergeyck - OneTech Graduate Program - TotalEnergies\", \"link\": \"https://be.linkedin.com/in/aymar-de-bergeyck-27361a189\", \"snippet\": \"I completed a two-year graduate program at TotalEnergies, where I had the opportunity to work in various departments over 8-month periods.\", \"position\": 3}, {\"title\": \"Family tree of Marie de BROUCHOVEN de BERGEYCK (1) - Geneanet\", \"link\": \"https://gw.geneanet.org/pierfit?lang=en&p=marie&n=de+brouchoven+de+bergeyck&oc=1\", \"snippet\": \"Aymar de POTTER d\\'INDOYE 1907-1982 Married in 1930 to Claire DAVIGNON 1910-2001 with : Private person · Private person · Private person · Private person ...\", \"position\": 4}, {\"title\": \"Maria Van Brouchoven Graaf van Bergeyck Family History ...\", \"link\": \"https://www.myheritage.com/names/maria_van%20brouchoven%20%20graaf%20van%20bergeyck\", \"snippet\": \"Birth. Maria Isabelle van Brouchoven de Bergeyck was born in 1926. Siblings. Maria had 6 brothers: Aymar van Brouchoven de Bergeyck, Jacob van Brouchoven de ...\", \"position\": 5}, {\"title\": \"Anne Michèle Marie de Liedekerke (1944 - 2004) - Genealogy - Geni\", \"link\": \"https://www.geni.com/people/Anne-Mich%C3%A8le-Marie-de-Liedekerke/6000000037517135446\", \"snippet\": \"Anne Michèle Marie de Liedekerke. Birthdate: April 22, 1944. Birthplace: Uccle, Brussels, Belgium. Death: October 17, 2004 (60)\", \"date\": \"May 2, 2022\", \"position\": 6}, {\"title\": \"Aymar de Potter d\\'Indoye : Family tree by gounou - Geneanet\", \"link\": \"https://gw.geneanet.org/gounou?lang=en&n=de+potter+d+indoye&p=aymar\", \"snippet\": \"Aymar de Potter d\\'Indoye. Born August 16, 1907 - Melle, Oost-Vlaanderen, Belgique; Deceased. Parents.\", \"position\": 7}, {\"title\": \"100+ \\\\\"Aymar De\\\\\" profiles - LinkedIn\", \"link\": \"https://www.linkedin.com/pub/dir/Aymar/De\", \"snippet\": \"Aymar De Bergeyck. Data Scientist @TotalEnergies. Brussels Metropolitan Area ... French school of spycho bio therapy, +1 more ...\", \"position\": 8}, {\"title\": \"Baronne Pauline van der Straten Waillet - Geni\", \"link\": \"https://www.geni.com/people/Baronne-Pauline-van-der-Straten-Waillet/6000000021908728362\", \"snippet\": \"Birth of Aymar Joseph Paul Marie Ghislain de Potter ... Melle, Oost-Vlaanderen, Vlaanderen, Belgium. 1960. September 2, 1960. Age 84. Death of ...\", \"date\": \"May 25, 2018\", \"position\": 9}], \"credits\": 1}', name='GoogleSerperAPIWrapper', id='7acdbfbc-5356-496a-a37b-7fb683c959a3', tool_call_id='call_lUUuVbkOPvNOUpHQv0N3TfNp'), AIMessage(content='Aymar de Bergeyck was born on April 22, 1944.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'service_tier': 'default'}, name='supervisor_agent', id='run--94ec44e6-5ac1-40db-9eb7-311b61016c00-0')]}\n"
     ]
    }
   ],
   "source": [
    "# Testing AI Agent on sample questions\n",
    "\n",
    "question = \"when is aymar de bergeyck born?\"\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "graph = create_plan_and_execute_agent(\n",
    "    llm_name_planner=\"gpt-4.1-mini\",\n",
    "    llm_name_executor=\"gpt-4.1-mini\",\n",
    "    llm_name_replanner=\"gpt-4.1-mini\",\n",
    "    llm_name_answer=\"gpt-4.1-mini\")\n",
    "\n",
    "initial_state = PlanExecute(\n",
    "    question=question,\n",
    "    plan=[],\n",
    "    intermediate_responses=[],\n",
    "    response=\"\",\n",
    "    current_step=0,\n",
    "    error_count=0,\n",
    "    validation=None,\n",
    "    agent_finished=False\n",
    ")\n",
    "\n",
    "workflow = initial_state\n",
    "workflow =  graph.invoke(initial_state, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155074c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: {'question': 'when is aymar de bergeyck born?', 'plan': ['Search the web for \"Aymar de Bergeyck birth date\" to find reliable sources that provide his date of birth.'], 'intermediate_responses': ['Aymar de Bergeyck was born on April 22, 1944.'], 'response': 'FINAL ANSWER: April 22 1944', 'current_step': 1, 'error_count': 0, 'validation': None, 'agent_finished': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Response:\", workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you deactivated any VPN like Tailscale, else some URLs will be blocked!\n",
      "created new folder\n",
      "beforemain\n",
      "In main()\n",
      "Starting run with arguments: Namespace(concurrency=8, model_id='o1', run_name='test1', set_to_run='validation', use_open_models=False, use_raw_dataset=False)\n",
      "gaia dataset path doesnt exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 161 files:   2%|▏         | 3/161 [00:05<04:23,  1.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 440\u001b[39m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll tasks processed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    439\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mbeforemain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 423\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    420\u001b[39m args = parse_args()\n\u001b[32m    421\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting run with arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m eval_ds = \u001b[43mload_gaia_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_raw_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_to_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded evaluation dataset:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    425\u001b[39m \u001b[38;5;28mprint\u001b[39m(pd.DataFrame(eval_ds)[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m].value_counts())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mload_gaia_dataset\u001b[39m\u001b[34m(use_raw_dataset, set_to_run)\u001b[39m\n\u001b[32m    126\u001b[39m         snapshot_download(\n\u001b[32m    127\u001b[39m             repo_id=\u001b[33m\"\u001b[39m\u001b[33mgaia-benchmark/GAIA\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    128\u001b[39m             repo_type=\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m             local_dir=\u001b[33m\"\u001b[39m\u001b[33mdata/gaia\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m             ignore_patterns=[\u001b[33m\"\u001b[39m\u001b[33m.gitattributes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    131\u001b[39m         )\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    133\u001b[39m         \u001b[38;5;66;03m# WARNING: this dataset is gated: make sure you visit the repo to require access.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmolagents/GAIA-annotated\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/gaia\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.gitattributes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mREADME.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_file_paths\u001b[39m(row):\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mfile_name\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Perso_projects/basic_gaia_ai_agent/venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Perso_projects/basic_gaia_ai_agent/venv/lib/python3.13/site-packages/huggingface_hub/_snapshot_download.py:327\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    325\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Perso_projects/basic_gaia_ai_agent/venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Perso_projects/basic_gaia_ai_agent/venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Perso_projects/basic_gaia_ai_agent/venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "### IMPORTANT: EVALUATION SWITCHES\n",
    "\n",
    "# custom_role_conversions = {\"tool-call\": \"assistant\", \"tool-response\": \"user\"}\n",
    "\n",
    "# user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\"\n",
    "\n",
    "# BROWSER_CONFIG = {\n",
    "#     \"viewport_size\": 1024 * 5,\n",
    "#     \"downloads_folder\": \"downloads_folder\",\n",
    "#     \"request_kwargs\": {\n",
    "#         \"headers\": {\"User-Agent\": user_agent},\n",
    "#         \"timeout\": 300,\n",
    "#     },\n",
    "#     \"serpapi_key\": os.getenv(\"SERPAPI_API_KEY\"),\n",
    "# }\n",
    "\n",
    "# os.makedirs(f\"./{BROWSER_CONFIG['downloads_folder']}\", exist_ok=True)\n",
    "# print('created new folder')\n",
    "\n",
    "def load_gaia_dataset(use_raw_dataset: bool, set_to_run: str) -> datasets.Dataset:\n",
    "    if not os.path.exists(\"data/gaia\"):\n",
    "        print('gaia dataset path doesnt exist')\n",
    "        if use_raw_dataset:\n",
    "            print('in use raw dataset')\n",
    "            snapshot_download(\n",
    "                repo_id=\"gaia-benchmark/GAIA\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "        else:\n",
    "            # WARNING: this dataset is gated: make sure you visit the repo to require access.\n",
    "            snapshot_download(\n",
    "                repo_id=\"smolagents/GAIA-annotated\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "\n",
    "    def preprocess_file_paths(row):\n",
    "        if len(row[\"file_name\"]) > 0:\n",
    "            row[\"file_name\"] = f\"data/gaia/{set_to_run}/\" + row[\"file_name\"]\n",
    "        return row\n",
    "    \n",
    "    eval_ds = datasets.load_dataset(\n",
    "        \"data/gaia/GAIA.py\",\n",
    "        name=\"2023_all\",\n",
    "        split=set_to_run,\n",
    "    )\n",
    "\n",
    "    eval_ds = eval_ds.rename_columns({\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"})\n",
    "    eval_ds = eval_ds.map(preprocess_file_paths)\n",
    "    return eval_ds\n",
    "\n",
    "def append_answer(entry: dict, jsonl_file: str) -> None:\n",
    "    jsonl_path = Path(jsonl_file)\n",
    "    jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with append_answer_lock, open(jsonl_file, \"a\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(json.dumps(entry) + \"\\n\")\n",
    "    assert jsonl_path.exists(), \"File not found!\"\n",
    "    print(\"Answer exported to file:\", jsonl_path.resolve())\n",
    "\n",
    "def answer_single_question_V2(\n",
    "    example: dict, answers_file: str\n",
    ") -> None:\n",
    "    print('\\n\\n')\n",
    "    print('================================================================================================================')\n",
    "    print('example', example)\n",
    "\n",
    "    question = example[\"question\"]\n",
    "    document_inspection_tool = TextInspectorTool(\n",
    "            model=init_chat_model(\"gpt-4.1\", model_provider=\"openai\", temperature=0), \n",
    "            text_limit=10000)\n",
    "    visual_inspection_tool = VisualQATool(model=init_chat_model(\"gpt-4.1\", model_provider=\"openai\", temperature=0))\n",
    "\n",
    "    if example[\"file_name\"]:\n",
    "        if \".zip\" in example[\"file_name\"]:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use these attached files:\\n\"\n",
    "            prompt_use_files += get_zip_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "        else:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use this attached file:\\n\"\n",
    "            prompt_use_files += get_single_file_description(\n",
    "                example[\"file_name\"], example[\"question\"], visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "\n",
    "    \n",
    "        # if \".zip\" in example[\"file_name\"]:\n",
    "        #     folder_path = example[\"file_name\"].replace(\".zip\", \"\")\n",
    "        #     os.makedirs(folder_path, exist_ok=True)\n",
    "        #     shutil.unpack_archive(example[\"file_name\"], folder_path)\n",
    "        #     string_w_paths = \"\"\n",
    "        #     for root, dirs, files in os.walk(folder_path):\n",
    "        #         print('root', root)\n",
    "        #         print('dirs', dirs)\n",
    "        #         print('files', files)\n",
    "        #         for file in files:\n",
    "        #             file_path = os.path.join(root, file)\n",
    "        #             string_w_paths+= file_path+\"\\n\"\n",
    "        #     prompt_use_files = f\"\\n\\nTo solve the question above, you will have to use these attached files: {string_w_paths}\\n\" \n",
    "\n",
    "        # else:\n",
    "        #     prompt_use_files = f\"\\n\\nTo solve the question above, you will have to use these attached files: {example[\"file_name\"]}\\n\" \n",
    "        question += prompt_use_files\n",
    "    print(question)\n",
    "    \n",
    "    # CREATE AGENT\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    graph = create_plan_and_execute_agent(\n",
    "        llm_name_planner=\"gpt-4.1-mini\",\n",
    "        llm_name_executor=\"gpt-4.1-mini\",\n",
    "        llm_name_replanner=\"gpt-4.1-mini\",\n",
    "        llm_name_answer=\"gpt-4.1-mini\")\n",
    "\n",
    "    initial_state = PlanExecute(\n",
    "        question=question,\n",
    "        plan=[],\n",
    "        intermediate_responses=[],\n",
    "        response=\"\",\n",
    "        current_step=0,\n",
    "        error_count=0,\n",
    "        validation=None,\n",
    "        agent_finished=False\n",
    "    )\n",
    "\n",
    "    start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    try: \n",
    "        # Run Agent\n",
    "        print('Question:', question)\n",
    "        workflow = initial_state\n",
    "        workflow =  graph.invoke(initial_state, config=config)\n",
    "        # print('QUESTION')\n",
    "        # print(workflow[\"question\"])\n",
    "        # print(\"STEPS\")\n",
    "        # previous_steps_with_answers = \"\\n\".join(\n",
    "        #     f\"{i}. INSTRUCTION: {workflow['plan'][i]} ANSWER: {workflow['intermediate_responses'][i]}\\n\" for i in range(len(workflow['intermediate_responses']))\n",
    "        # )\n",
    "        # print(previous_steps_with_answers)\n",
    "        # print(\"Current step: \", workflow[\"current_step\"])\n",
    "        # print(\"RESPONSE\")\n",
    "        # print(workflow[\"response\"])\n",
    "        # print('TRUE RESPONSE')\n",
    "        # print(example['true_answer'])\n",
    "        # response = \"No response was found\"\n",
    "        # for event in graph.stream(initial_state, config=config):\n",
    "        #     if \"planner\" in event:\n",
    "        #         print('PLAN')\n",
    "        #         for i, p in enumerate(event['planner']['plan']):\n",
    "        #             print(f\"Step {i}: {p}\")\n",
    "        #     elif \"agent\" in event:\n",
    "        #         print('AGENT')\n",
    "        #         print(event['agent']['intermediate_responses'])\n",
    "        #     elif \"replan\" in event:\n",
    "        #         if 'plan' in event['replan']:\n",
    "        #             print('REPLAN')\n",
    "        #             for i, p in enumerate(event['replan']['plan']):\n",
    "        #                 print(f\"Step {i}: {p}\")\n",
    "        #         else:\n",
    "        #             print('REPLAN ERROR')\n",
    "        #             print(event['replan'])\n",
    "        #     elif 'answer' in event:\n",
    "        #         response = event['answer']['response']\n",
    "        #         print('RESPONSE')\n",
    "        #         print(response)\n",
    "        #         break\n",
    "        # workflow = {}\n",
    "\n",
    "        raised_exception = False\n",
    "    except Exception as e: \n",
    "        print(\"Error on\", question, e)\n",
    "        exception = e\n",
    "        raised_exception = True\n",
    "\n",
    "    end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    annotated_example = {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"augmented_question\": question,\n",
    "        \"response\": workflow[\"response\"].replace('FINAL ANSWER: ', ''),\n",
    "        \"plan\": workflow[\"plan\"],\n",
    "        \"intermediate_responses\": workflow[\"intermediate_responses\"],\n",
    "        \"current_step\": workflow[\"current_step\"],\n",
    "        \"plan_with_answers\": \"\\n\".join(\n",
    "            f\"{i}. INSTRUCTION: {workflow['plan'][i]} ANSWER: {workflow['intermediate_responses'][i]}\\n\" for i in range(len(workflow['intermediate_responses']))\n",
    "        ),\n",
    "        \"agent_error\": str(exception) if raised_exception else None,\n",
    "        \"task\": example[\"task\"],\n",
    "        \"task_id\": example[\"task_id\"],\n",
    "        \"true_answer\": example[\"true_answer\"],\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "    }\n",
    "    print('-------------------------------------------------------------')\n",
    "    list_print = ['question', 'augmented_question', 'plan_with_answers', 'response', 'true_answer']\n",
    "    for k in list_print:\n",
    "        print(k)\n",
    "        print(annotated_example[k])\n",
    "    print('-------------------------------------------------------------')\n",
    "    # print('annotated_example')\n",
    "    # print(annotated_example)\n",
    "    append_answer(annotated_example, answers_file)\n",
    "\n",
    "def get_examples_to_answer(answers_file: str, eval_ds: datasets.Dataset) -> list[dict]:\n",
    "    print(f\"Loading answers from {answers_file}...\")\n",
    "    try:\n",
    "        done_questions = pd.read_json(answers_file, lines=True)[\"question\"].tolist()\n",
    "        print(f\"Found {len(done_questions)} previous results!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error when loading records: \", e)\n",
    "        print(\"No usable records! ▶️ Starting new.\")\n",
    "        done_questions = []\n",
    "    return [line for line in eval_ds.to_list() if line[\"question\"] not in done_questions and line[\"file_name\"]]\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--concurrency\", type=int, default=8)\n",
    "    parser.add_argument(\"--run-name\", type=str, default=\"test1\")\n",
    "    parser.add_argument(\"--set-to-run\", type=str, default=\"validation\")\n",
    "    parser.add_argument(\"--use-raw-dataset\", action=\"store_true\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    print('In main()')\n",
    "    args = parse_args()\n",
    "    print(f\"Starting run with arguments: {args}\")\n",
    "\n",
    "    eval_ds = load_gaia_dataset(args.use_raw_dataset, args.set_to_run)\n",
    "    print(\"Loaded evaluation dataset:\")\n",
    "    print(pd.DataFrame(eval_ds)[\"task\"].value_counts())\n",
    "\n",
    "    answers_file = f\"output/{args.set_to_run}/{args.run_name}.jsonl\"\n",
    "    tasks_to_run = get_examples_to_answer(answers_file, eval_ds)\n",
    "    print('answer file', answers_file)\n",
    "    # Sequential processing with tqdm progress bar\n",
    "\n",
    "    # example =  {'task_id': '5b2a14e8-6e59-479c-80e3-4696e8980152', 'question': 'The brand that makes these harnesses the dogs are wearing in the attached pic shares stories from their ambassadors on their website. What meat is mentioned in the story added Dec 8th 2022?', 'task': '3', 'true_answer': 'bacon', 'file_name': 'data/gaia/validation/5b2a14e8-6e59-479c-80e3-4696e8980152.jpg', 'file_path': '/Users/aymar/Documents/Perso_projects/Agents-course/unit4-basic-agent/tools/data/gaia/2023/validation/5b2a14e8-6e59-479c-80e3-4696e8980152.jpg', 'Annotator Metadata': {'Steps': '1. Use image search for \"dog harness brands with yellow logos\"\\n2. Look at harnesses until a similar harness shows up\\n3. Click through to see the harness\\n4. Search \"ruffwear\"\\n5. Go to the website\\n6. Navigate to stories\\n7. Find the story posted Dec 8th 2022\\n8. Read the story to find any meats mentioned', 'Number of steps': '8', 'How long did this take?': '15 minutes', 'Tools': '1. image recognition tools\\n2. image search tools\\n3. web browser\\n4. search engine', 'Number of tools': '4'}}\n",
    "    # answer_single_question_V2(example, answers_file)\n",
    "\n",
    "    for example in tqdm(tasks_to_run, desc=\"Processing tasks\"):\n",
    "        answer_single_question_V2(example, answers_file)\n",
    "\n",
    "    print(\"All tasks processed.\")\n",
    "print('beforemain')\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b014b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
