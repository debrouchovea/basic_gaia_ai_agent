{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "from tools.reformulator import prepare_response\n",
    "from tools.run_agents import get_single_file_description, get_zip_description\n",
    "from tools.text_inspector_tool import TextInspectorTool\n",
    "from tools.text_web_browser import (\n",
    "    ArchiveSearchTool,\n",
    "    FinderTool,\n",
    "    FindNextTool,\n",
    "    PageDownTool,\n",
    "    PageUpTool,\n",
    "    SimpleTextBrowser,\n",
    "    VisitTool,\n",
    ")\n",
    "from tools.visual_qa import VisualQATool\n",
    "from tools.agent_with_tools import create_plan_and_execute_agent, PlanExecute\n",
    "\n",
    "from smolagents import (\n",
    "    CodeAgent,\n",
    "    GoogleSearchTool,  # make sure this is distinct from Serp/BSerp tools\n",
    "    LiteLLMModel,\n",
    "    Model,\n",
    "    ToolCallingAgent,\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model, ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, SerpAPIWrapper\n",
    "from langchain_community.tools import BraveSearch\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "append_answer_lock = threading.Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing AI Agent on sample questions\n",
    "\n",
    "question = \"when is aymar de bergeyck born?\"\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "graph = create_plan_and_execute_agent(\n",
    "    llm_name_planner=\"gpt-4.1-mini\",\n",
    "    llm_name_executor=\"gpt-4.1-mini\",\n",
    "    llm_name_replanner=\"gpt-4.1-mini\",\n",
    "    llm_name_answer=\"gpt-4.1-mini\")\n",
    "\n",
    "initial_state = PlanExecute(\n",
    "    question=question,\n",
    "    plan=[],\n",
    "    intermediate_responses=[],\n",
    "    response=\"\",\n",
    "    current_step=0,\n",
    "    error_count=0,\n",
    "    validation=None,\n",
    "    agent_finished=False\n",
    ")\n",
    "\n",
    "workflow = initial_state\n",
    "workflow =  graph.invoke(initial_state, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Response:\", workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_gaia_dataset(use_raw_dataset: bool, set_to_run: str) -> datasets.Dataset:\n",
    "    if not os.path.exists(\"data/gaia\"):\n",
    "        print('gaia dataset path doesnt exist')\n",
    "        if use_raw_dataset:\n",
    "            print('in use raw dataset')\n",
    "            snapshot_download(\n",
    "                repo_id=\"gaia-benchmark/GAIA\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "        else:\n",
    "            # WARNING: this dataset is gated: make sure you visit the repo to require access.\n",
    "            snapshot_download(\n",
    "                repo_id=\"smolagents/GAIA-annotated\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=\"data/gaia\",\n",
    "                ignore_patterns=[\".gitattributes\", \"README.md\"],\n",
    "            )\n",
    "\n",
    "    def preprocess_file_paths(row):\n",
    "        if len(row[\"file_name\"]) > 0:\n",
    "            row[\"file_name\"] = f\"data/gaia/{set_to_run}/\" + row[\"file_name\"]\n",
    "        return row\n",
    "    \n",
    "    eval_ds = datasets.load_dataset(\n",
    "        \"data/gaia/GAIA.py\",\n",
    "        name=\"2023_all\",\n",
    "        split=set_to_run,\n",
    "    )\n",
    "\n",
    "    eval_ds = eval_ds.rename_columns({\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"})\n",
    "    eval_ds = eval_ds.map(preprocess_file_paths)\n",
    "    return eval_ds\n",
    "\n",
    "def append_answer(entry: dict, jsonl_file: str) -> None:\n",
    "    jsonl_path = Path(jsonl_file)\n",
    "    jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with append_answer_lock, open(jsonl_file, \"a\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(json.dumps(entry) + \"\\n\")\n",
    "    assert jsonl_path.exists(), \"File not found!\"\n",
    "    print(\"Answer exported to file:\", jsonl_path.resolve())\n",
    "\n",
    "def answer_single_question(\n",
    "    example: dict, answers_file: str\n",
    ") -> None:\n",
    "    print('EXAMPLE', example)\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    document_inspection_tool = TextInspectorTool(\n",
    "            model=init_chat_model(\"o3\", model_provider=\"openai\", temperature=0), \n",
    "            text_limit=10000)\n",
    "    visual_inspection_tool = VisualQATool(model=init_chat_model(\"o3\", model_provider=\"openai\", temperature=0))\n",
    "\n",
    "    if example[\"file_name\"]:\n",
    "        if \".zip\" in example[\"file_name\"]:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use these attached files:\\n\"\n",
    "            prompt_use_files += get_zip_description(\n",
    "                example[\"file_name\"], None, visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "        else:\n",
    "            prompt_use_files = \"\\n\\nTo solve the task above, you will have to use this attached file:\\n\"\n",
    "            prompt_use_files += get_single_file_description(\n",
    "                example[\"file_name\"], None, visual_inspection_tool, document_inspection_tool\n",
    "            )\n",
    "\n",
    "        question += prompt_use_files\n",
    "    \n",
    "    # CREATE AGENT\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    graph = create_plan_and_execute_agent(\n",
    "        llm_name_planner=\"o3\",\n",
    "        llm_name_executor=\"o3\",\n",
    "        llm_name_replanner=\"o3\",\n",
    "        llm_name_answer=\"o3\",\n",
    "        llm_text_inspector = \"o3\",\n",
    "        llm_visual_qa = \"o3\"\n",
    "        )\n",
    "\n",
    "    initial_state = PlanExecute(\n",
    "        question=question,\n",
    "        plan=[],\n",
    "        intermediate_responses=[],\n",
    "        response=\"\",\n",
    "        current_step=0,\n",
    "        error_count=0,\n",
    "        validation=None,\n",
    "        agent_finished=False\n",
    "    )\n",
    "\n",
    "    start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    try: \n",
    "        # Run Agent\n",
    "        workflow = initial_state\n",
    "        workflow =  graph.invoke(initial_state, config=config)\n",
    "        raised_exception = False\n",
    "\n",
    "    except Exception as e: \n",
    "        print(\"Error on\", question, e)\n",
    "        exception = e\n",
    "        raised_exception = True\n",
    "\n",
    "    end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    annotated_example = {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"augmented_question\": question,\n",
    "        \"response\": workflow[\"response\"].replace('FINAL ANSWER: ', ''),\n",
    "        \"plan\": workflow[\"plan\"],\n",
    "        \"intermediate_responses\": workflow[\"intermediate_responses\"],\n",
    "        \"current_step\": workflow[\"current_step\"],\n",
    "        \"plan_with_answers\": \"\\n\".join(\n",
    "            f\"{i}. INSTRUCTION: {workflow['plan'][i]} ANSWER: {workflow['intermediate_responses'][i]}\\n\" for i in range(len(workflow['intermediate_responses']))\n",
    "        ),\n",
    "        \"agent_error\": str(exception) if raised_exception else None,\n",
    "        \"task\": example[\"task\"],\n",
    "        \"task_id\": example[\"task_id\"],\n",
    "        \"true_answer\": example[\"true_answer\"],\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "    }\n",
    "    print('-------------------------------------------------------------')\n",
    "    list_print = ['question', 'augmented_question', 'plan_with_answers', 'response', 'true_answer']\n",
    "    for k in list_print:\n",
    "        print(k)\n",
    "        print(annotated_example[k])\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    append_answer(annotated_example, answers_file)\n",
    "\n",
    "\n",
    "def get_examples_to_answer(answers_file: str, eval_ds: datasets.Dataset) -> list[dict]:\n",
    "    print(f\"Loading answers from {answers_file}...\")\n",
    "    try:\n",
    "        done_questions = pd.read_json(answers_file, lines=True)[\"question\"].tolist()\n",
    "        print(f\"Found {len(done_questions)} previous results!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error when loading records: \", e)\n",
    "        print(\"No usable records! ▶️ Starting new.\")\n",
    "        done_questions = []\n",
    "    return [line for line in eval_ds.to_list() if line[\"question\"] not in done_questions and line[\"file_name\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"validation_level_1_V1\"\n",
    "set_to_run = \"validation\"  # or \"test\" or \"train\"\n",
    "use_raw_dataset = True\n",
    "levels_to_run = [1]\n",
    "eval_ds = load_gaia_dataset(use_raw_dataset, set_to_run)\n",
    "\n",
    "print(\"Loaded evaluation dataset:\")\n",
    "print(pd.DataFrame(eval_ds)[\"task\"].value_counts())\n",
    "\n",
    "answers_file = f\"output/{set_to_run}/{run_name}.jsonl\"\n",
    "tasks_to_run = get_examples_to_answer(answers_file, eval_ds)\n",
    "\n",
    "for example in tqdm(tasks_to_run, desc=\"Processing tasks\"):\n",
    "    if int(example[\"task\"]) in levels_to_run:\n",
    "        if example['task_id'] == 'cca530fc-4052-43b2-b130-b30968d8aa44':\n",
    "            continue\n",
    "        else:\n",
    "            answer_single_question(example, answers_file)\n",
    "\n",
    "# example = {'task_id': '1f975693-876d-457b-a649-393859e79bf3', 'question': \"Hi, I was out sick from my classes on Friday, so I'm trying to figure out what I need to study for my Calculus mid-term next week. My friend from class sent me an audio recording of Professor Willowbrook giving out the recommended reading for the test, but my headphones are broken :(\\n\\nCould you please listen to the recording for me and tell me the page numbers I'm supposed to go over? I've attached a file called Homework.mp3 that has the recording. Please provide just the page numbers as a comma-delimited list. And please provide the list in ascending order.\", 'task': '1', 'true_answer': '132, 133, 134, 197, 245', 'file_name': 'data/gaia/validation/1f975693-876d-457b-a649-393859e79bf3.mp3', 'file_path': '/Users/aymar/Documents/Perso_projects/Agents-course/unit4-basic-agent/tools/data/gaia/2023/validation/1f975693-876d-457b-a649-393859e79bf3.mp3', 'Annotator Metadata': {'Steps': 'Step 1: Load the file supplied by my user.\\nStep 2: Using audio processing tools, convert the text of the audio file to speech:\\n\\n\"Before you all go, I want to remind you that the midterm is next week. Here\\'s a little hint; you should be familiar with the differential equations on page 245, problems that are very similar to problems 32, 33, and 44 from that page might be on the test. And also some of you might want to brush up on the last page in the integration section, page 197. I know some of you struggled on last week\\'s quiz. I foresee problem 22 from page 197 being on your midterm. Oh, and don\\'t forget to brush up on the section on related rates, on pages 132, 133, and 134.\"\\n\\nStep 3: Evaluate the converted audio, recording each instance of page numbers: 245, 197, 197, 132, 133, 134\\nStep 4: Sort the page numbers in ascending order, omitting duplicates, and store this list as the correct answer to my user\\'s request: 132, 133, 134, 197, 245\\nStep 5: Report the correct response to my user: \"132, 133, 134, 197, 245\"', 'Number of steps': '5', 'How long did this take?': '2 minutes', 'Tools': '1. A file interface\\n2. A speech-to-text audio processing tool', 'Number of tools': '2'}}\n",
    "# answer_single_question(example, answers_file)\n",
    "print(\"All tasks processed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the file (assuming one JSON object per line)\n",
    "lines = []\n",
    "with open(\"./output/validation/testlevel2.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line))\n",
    "# Pretty-print each object\n",
    "count_correct = 0\n",
    "for obj in lines:\n",
    "    print(json.dumps(obj, indent=2))\n",
    "    if obj['response'] == obj['true_answer']:\n",
    "        count_correct += 1\n",
    "print(f\"Number of correct answers: {count_correct} out of {len(lines)}\")\n",
    "print(f\"Accuracy: {count_correct / len(lines) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./output/validation/test6.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        print(json.dumps(obj, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
